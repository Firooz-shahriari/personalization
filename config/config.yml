# Configuration file for the personalized DAGP project
# This file contains settings for the experiment, including dataset, model, and optimizer configurations.

# experiment configuration
reproducibility: True  # Whether to ensure reproducibility of results
seed: 42  # Random seed for reproducibility

# graph configuration
graph:
  type: "random"  # Options: "random", "grid", "star"
  directed: True  # Whether the graph is directed or undirected
  num_nodes: 15  # Number of nodes in the graph
  edge_prob: 1.  # Probability of edge creation between nodes
  laplacian_factor: 2.0  # Factor for Laplacian matrix scaling

# problem configuration
problem:
  type: "synthetic" # Options: "synthetic", "LogisticRegression" 
  synthetic:
    local_dimension: 10  # dimension of local variables
    global_dimension: 10  # dimension of global variables
    epsilon: 0.01  # Epsilon value of constraints
    global_objective_exists: True  # Whether the global objective exists
  # logistic_regression:
  #   limited_labels: 10  # Number of labels for Logistic Regression
  #   balanced: True  # Whether the dataset is balanced or not
  #   train_size: 1000  # Size of the training set
  #   regularization: True  # Whether to use regularization
  #   regularization_strength: 0.01  # Strength of the regularization

# optimizer configuration
optimizer:
  p_dagp:
    max_iter: 8000  # Maximum number of iterations for personalized dagp
    alpha: 0.1  # Alpha parameter for personalized dagp
    rho: 0.1  # Rho parameter for personalized dagp
    learning_rate:
      initial: 0.01  # Initial learning rate
      min: 0.00001    # Minimum learning rate
      decay_rate: 1.  # Decay rate for learning rate scheduling
      decay_steps: 100   # Number of steps between learning rate updates
  pcgd:
    max_iter: 4000  # Maximum number of iterations for cdd
    projection_iters: 5  # Number of projection iterations  
    learning_rate:
      initial: 0.05  # Initial learning rate
      min: 0.00001    # Minimum learning rate
      decay_rate: 1.  # Decay rate for learning rate scheduling
      decay_steps: 100   # Number of steps between learning rate updates

